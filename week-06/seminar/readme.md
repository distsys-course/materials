# Семинар 6. Group membership и детектор отказов.

<p align="center"><img src="./media/failure-detector.jpg" width="50%" /></p>

В распределенных системах нужно постоянно мониторить отказы. Необходимость
очевидна: чем быстрее и точнее мы понимаем, что сломалось -- тем быстрее мы
можем это починить. В рамках этого семинара мы рассмотрим устройство SWIM и
Lifeguard -- двух популярных детекторов.

## Виды отказов

Для начала определим модель отказов, которые мы хотим детектить. Выделяют два
больших класса:

- Византийские отказы (bysantinne failure) -- узел системы может отправлять в
сеть некорректные с семантической или синтаксической точек зрения сообщения
- "Взрывные отказы" (stop failure) -- узел системы перестает работать и больше
в систему не возвращается

Пара слов о византийских отказах: их *гораздо* сложнее обнаруживать. Для системы,
состоящей из 3f + 1 узлов, мы можем поймать только f отказов, причем доказано,
что уже f+1 узел может обмануть любой теоретически возможный детектор.

Важно понимать, что детекторы отказов не отличают сбойный узел от узла, который
потерял сетевую связность с другими узлами, от узла, который остановился.

Сегодня мы разберем принципы, позволяющие строить детекторы отказов, а также
посмотрим на внутреннее устройство протоколов SWIM и Lifeguard. При этом мы
оставим за скобками вопрос византийских отказов.

Есть и более сложные модели отказов, которые мы не рассматриваем в рамках этого курса. Например:

1. [Visigoth Fault Tolerance](https://www.researchgate.net/publication/282069314_Visigoth_fault_tolerance)
1. [Gray Failure: The Achilles’ Heel of Cloud-Scale Systems](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-1.pdf)

## Причины отказов

Предлагаем почитать следующие статьи:

1. [Why Does the Cloud Stop Computing? Lessons from Hundreds of Service Outages](https://ucare.cs.uchicago.edu/pdf/socc16-cos.pdf)
1. [What bugs cause production cloud incidents?](https://people.cs.uchicago.edu/~shanlu/paper/hotos19_azure.pdf)

Для более детального погружения можно разобрать конкретные [постмортемы](https://github.com/danluu/post-mortems).

## А что по доступности?

Часто можно увидеть заявления об уровне доступности в 99.999%, отчего может
возникнуть вопрос: а почему бы не поставить 100%? Ответ: к сожалению, это
недостижимая отметка -- железо, которое никогда не отказывает стоит слишком
много для практического использования. Для справки можете почитать книжку о SRE
в компании Google, она действительно покрывает многие аспекты реального опыта
и подробно объясняет, почему 100% невозможно достичь. 

Ссылки:

1. [Thinking about Availability in Large Service Infrastructures](https://research.google/pubs/thinking-about-availability-in-large-service-infrastructures/) -- опыт в компании Google
1. [SRE Books](https://sre.google/books/)

## Как можно было бы реализовать такой сервис?

Есть несколько вариантов, какой протокол можно использовать для определения отказов в РС.

Первым и самым примитивным является механизм ping-pong запросов:

<p align="center"><img src="./media/ping-basic.jpg" width="50%" /></p>

Преимущества -- простая реализация. Недостатки -- большая нагрузка на сеть.

## SWIM

<p align="center"><img src="./media/swim-basic.jpg" width="50%" /></p>

*S*calable *W*eakly-consistent *I*nfection-style Process Group *M*embership
Protocol -- протокол, впервые представленный публике в 2002 году. В то время
масштабы были совершенно иные и только единичные институты и компании могли
иметь около 100+ компьютеров, в статье примеры приведены только лишь на кластере
из 55 машин, что в то время являлось огромным достижением.

Важно, что детекторы можно строить с weakly-consistent (то есть когда-нибудь
машины кластера узнают) гарантиями.

SWIM в своём основании имеет алгоритм Gossip, но только спрашивается ровно один
процесс. Каждый узел A выбирает одного случайного соседа B, которому он
отправляет сообщение ping. Узел B, получивший такое сообщение, в ответ
отправляет сообщение ack. Если же A видит ответ узла B, то B считается живым
процессом. Такие раунды проходят каждые $T$ 
времени, этот таймаут фиксирован в протоколе и никогда не меняется, даже если
сеть начинает деградировать повсеместно, эта одна из отличительных особенностей
протокола, о недостатках которой мы поговорим ниже.

Каждый процесс поддерживает список всех известных для него участников системы.
В отличие от heartbeats, где каждый узел посылает каждому узлу сообщение о том,
что он жив, количество сообщений за один раунд в SWIM растет линейно от
количества узлов.

В случае, если ответ не дошел, узел, отправивший изначально запрос, инициирует
"дозвон" остальных узлов. В протоколе зафиксировано число k -- количество узлов,
которым будет отправлен запрос ping-req. При получении этого запроса, машина
должна отправить ping машине под вопросом и ретранслировать ответ обратно. На
практике k выбирают константным -- оно выбрано так, потому что если процесс
мёртв и не отвечает хотя бы двум независимым, то с большой вероятностью что-то
не так с самим процессом. Авторы статьи предлагают брать k равным трём.

<p align="center"><img src="./media/swim-ping-req.jpg" width="50%" /></p>

В случае, если indirect pings, не сработали, то машина под вопросом объявляется
отказавшей, и информация об этом отправляется всем остальным посредствам
какого-нибудь gossip протокола (в простейшем случае -- IP Multicast).

<p align="center"><img src="./media/swim-multicast.jpg" width="50%" /></p>

Посчитаем вероятности. Пусть $q_f$ -- 
дробная часть количества неотказавших процессов, n --
количество всего процессов в системе. Также стоит считать, что это число больше
какой-нибудь константы, на практике, если примерно 20-30% всего оборудования
не работают, это считают проблемой доступности железа и системы отказов как правило
сыпят огромным количеством предупреждений. Также предположим, что у всех
процессов есть список всех других из n
узлов. Тогда вероятность того, что данный процесс будет выбран в качестве ping
равна:

$$1-(1 - \frac{1}{n - 1})^{nq_f}$$

Так происходит, потому что всего живо $nq_f$ процессов,
вероятность каждого выбрать не данного равна $1 - \frac{1}{n - 1}$,
значит для выбора данного надо вычесть это число из единицы.

Отсюда имеем следующее среднее время детекции отказа при достаточно больших n:

$$\frac{T}{1 - e^{-q_f}}$$

При этом, каждый корректный узел может быть объявлен отказавшим из-за потери
пакетов сетью. В статье авторы приводят следующую формулу при вероятности доставки
пакета равной
$q_{ml}$
(это число должно быть достаточно большим, на практике рассчитывают от 0.9-0.95):

$$q_f(1 - q_{ml}^2)(1 - q_fq_{ml}^4)^k\frac{1}{1 - e^{-q_f}}$$

Вывод этой формулы Вы можете посмотреть [здесь](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.4379&rep=rep1&type=pdf),
там ничего сложного, предел и простая логика о том, что ping не ответил и все из
k упавшие или не доставили хотя бы один из 4 процессов. Последний множитель
отвечает за время, сколько мы должны ждать, чтобы процесс был выбран как ping target.

Из этой формулы стоит подбирать k, например, если 90% процессов работают при
90% доступности сети, то вероятность ложноположительного обнаружения хорошего
процесса как упавшего равна около 0.02 при k=3. В целом 1-2% это достаточно
стандартные числа для классического SWIM протокола в обнаружении
ложноположительных результатов.

Когда какой-то процесс замечает, что другой узел является упавшим, то этот процесс
начинает рассылать вместе с ping и ping-req запросами информацию о том, кого
процесс считает упавшим. Это сделано для того, чтобы об этом обнаружении узнали
остальные участники системы, раунды Gossip позволяют распространять эту информацию
достаточно быстро. Эта информация распространяется по дифференциальному
уравнению по времени t и математическое ожидание равно:

$$\frac{dx}{dt} = \left(\frac{2}{n} - \frac{1}{n^2}\right)x(n - x) \Rightarrow x = \frac{n}{1 %2B (n - 1)e^{-(2 - 1/n)t}}$$

Относитесь к этой формуле как к скорости распространения по разрезу с вероятностью
соединения упавшего (инфицированного) и неупавшего процесса. Получается, что
чтобы практически все знали об инфицированном достаточно $O(\log n)$ раундов, на практике берут строго логарифм и SWIM за это время сходится с
большой вероятностью.

Другим доказательством такой асимптотики является тот факт, что случайный граф,
где из каждой вершины выходит $2 \log n$ случайных
ребёр из каждой вершины с большой вероятностью будет
[связным](https://mathoverflow.net/questions/60075/connectivity-of-the-erd%C5%91s-r%C3%A9nyi-random-graph).
Математические доказательства мы оставляем на курс дополнительных глав по теории
вероятностей, который проходит на ФКН уже как 4 года.

В целом при интервале в 0.2 секунды и с 10% потерей пакетов и 10% упавшими
узлами сходимость SWIM составит около 3.5 секунд, когда все узнают о состоянии
всех других узлов:

<p align="center"><img src="./media/swim-convergence.png" width="80%" /></p>

## Suspicion Mechanism

Так как железо нестабильно, и иногда узлы могут просто уходить в себя на
несколько секунд или просто срабатывать ложноположительные отказы из-за сети,
то хочется как-то уменьшать такие проблемы. Конечно, можно увеличивать k, но
такое решение спасает только от проблемы нестабильности сети, но не от проблемы
того, что процесс решит подумать какое-то время. Для этого SWIM (и в последствии
Lifeguard) вводит Suspicion механизм, когда другие узлы не сразу считают
неответивший процесс упавшим, а "подозревают" его какое-то время. То есть если
процесс A не получил подтверждение от процесса B, то A не считает B сразу
упавшим, а только подозревает его, но и одновременно рассылает эту информацию
всем остальным при ping и ping-req запросах, что A подозревает B. Все остальные
процессы, которые получили такое сообщение, тоже помечают B подозрительным.
Подозрительные процессы считаются неупавшими, но если к ним задают ping запрос,
и они отвечают, то рассылается всем Alive сообщение.

Далее у подозрительного процесса B есть время $T'$, которое
используется для того, чтобы он ответил на ping утвердительно -- показал, что
он жив. Это время берётся с запасом, чтобы знать с большой вероятностью, что
кто-то сделает ping запрос к этому упавшему процессу.

1. Если процесс B показал, что он жив процессу C, то C начинает распространять
эту информацию всем остальным (Alive).
1. Если процесс B в течение таймаута не ответил процессу, то начинается
распространение подтверждения информации, о том, что процесс мёртв (Confirm).

У такого подхода есть проблема, что некоторые процессы могут не успеть получить
подтверждение и считать процесс B мёртвым (и даже начинать это распространять).
Для решения этой проблемы каждый процесс хранит в своём membership list ещё
числа о других процессах, которые называют reincarnation numbers, они отвечают
за эпохи, когда сами процессы возрождаются. То есть каждый процесс хранит это
число у себя и только его может инкрементировать, как только ему надо ответить
на сообщение ping, он инкрементирует это число и отвечает вместе с ним. Alive сообщения тоже
распространяются всей системе через ping, ping-req запросы. Так как в
ответе есть incarnation number, то при распространении Alive, оно нивелирует
все остальные сообщения с меньшими эпохами, сообщения Suspect нивелирует
все "меньшие" Suspect и "не большие" Alive, а также все Confirm. А если уж пришло
Confirm и не пришло никакое Alive до этого или Suspect, то процесс всё таки будет
считаться умершим. Выбирать T' стоит как $C T \log n$
для достаточной константы C, чтобы знать, что сообщения Suspect всем доставятся
и не раньше, чем начнётся распространение Confirm.

Такой механизм позволяет уменьшать ложноположительные случаи детектора, но в то
же время могут увеличивать время, когда упавший процесс правильно обнаруживается
как упавший. Стоит отметить, что это является расширением протокола, а не его
обязательной частью. Часто в реализациях этого протокола эту часть опускают из-за
сложности.

SWIM долгое время (да и сейчас) является стандартом в обнаружении отказов в weak
consistency модели, например, система [Tarantool](https://slides.com/gerold103/swim-fosdem2020)
использует её.

В инженерной системе [Serf](https://github.com/hashicorp/serf) этот алгоритм был долгое время
основным методом обнаружения отказов. Было достаточно случаев жалоб пользователей,
что при деградации сети или DDOS атаке SWIM не показывал себя лучшим образом.
Многие процессы начинали мигать то рабочими, то нерабочими, и SWIM не
подстраивается под такие аномалии никак, в том числе потому что у этого
протокола не существует никакого динамического изменения времен, и такой
протокол очень уязвим к временным сложностям в сетевом взаимодействии. Поэтому
разработчики Serf пришли к своему протоколу, который они назвали Lifeguard.

## Lifeguard

Lifeguard в своей основе содержит SWIM с Suspicion Mechanism с некоторыми
модификациями. Во-первых, на запросы ping-req при истечении 80% таймаута запроса
будет возвращаться nack сообщение. Оно показывает, что что-то долго подозрительный
процесс отвечает. Если пришёл ack после nack, то nack нивилируется.
Во-вторых, каждый узел хранит о себе специальное число, которое
называет LHM (Local Health Multiplier), то есть это просто число, которое
пытается понять о своём здоровье. Это число не может уйти меньше 0, больше
S (авторы статьи предлагают брать число 8), и по умолчанию 0. Изменения с этим
числом происходят так:

1. -1, если нам ответили ack на ping или ping-req (всё хорошо)
1. +1, если ping не получил ответа (может быть проблема в нас?)
1. +1, если нас заподозрили в падении.
Для этого нам надо ещё подкорректировать Suspicion Mechanism в том плане, что
если кто-то нам задаёт запрос ping, то нам сообщают о том, что мы подозрительные.
1. +1, если на ping-req нам не пришёл nack (значит до нас не дошло никакое подтверждение)

Этот множитель линейно увеличивает интервал того, с какой частотой мы запрашиваем
другие процессы об их живости, то есть просто $T \cdot (LHM(S) %2B 1)$
а также увеличиваем таймаут для того, чтобы ждать ответов ack на запросы по ping,
ping-req так же линейно. Такой подход помогает лучше понимать своё состояние и
достаточно быстро подстраиваться под оптимальные таймауты в зависимости от состояния сети
на узле.

Такая эвристика называется Local Health Aware Probe.

Другой эвристикой является небольшая модификация Suspicion Mechanism: мы увеличим
таймаут при подозрении процесса A и как только другие процессы начнут также
подозревать A, начнём этот таймаут уменьшать, так как если уж и другие стали
подозревать процесс, скорее всего проблема стабильна. Также мы введём
минимальные и максимальные таймауты, и авторы статьи предлагают уменьшать
таймаут с логарифмическим шагом:

$$SuspicionTimeout = \max(Min, Max - (Max - Min)\frac{\log(C %2B 1)}{\log(k %2B 1)})$$

Где C -- количество подтверждающих процессов о том, что процесс A подозрительный.
Min и Max авторы советуют брать как $5 \log 10(n) T$ и
$6 Min$ соответственно. Интуицию за константами
можете посмотреть в оригинальной [статье](https://arxiv.org/pdf/1707.00788.pdf).

Эту эвристику авторы назвали Dogpile.

И последняя эвристика, в которой авторы SWIM скорее всего не увидели
сильного профита -- рассылать suspicion идентификатор при запросе ping. Как уже
показано выше в Local Health Aware Probe это нужно для лучшего подстраивания
таймаутов и на 1 раунд более быстрого распространения Alive сообщения.

Эта эвристика называется Buddy.

В итоге получается сильное сокращение ложноположительных результатов (>98%),
как и по отдельности, так и вместе при практически любых конфигурациях падений:

<p align="center"><img src="./media/lifeguard-fp.png" width="80%" /></p>

Понятное дело, что количество сообщений у Lifeguard протокола увеличится,
так как надо посылать обратно nack. Время обнаружения тоже должно увеличиться, потому что надо дольше ждать,
чем в стандартном Suspicion Mechanism. Автор статьи в докладе рассказывает, что на медиане
наблюдалось только 2% замедление обнаружения отказов, а также 7% и 16% на 99 и
99.9 перцентилях. А количество сообщений и данных увеличилось в среднем на
12% при достаточно нестабильной сети >20% падений и на только 3%, если система
достаточно стабильна (не более 1% падений).

Этот протокол был внедрён по умолчанию в Serf и Consul и хорошо себя
зарекомендовал. Советуем посмотреть [доклад](https://www.youtube.com/watch?v=u-a7rVJ6jZY&feature=youtu.be)
от одного из автора статьи про внедрение этого алгоритма в Serf.

## Ещё про детекторы отказов

1. [The phi Accrual Failure Detector](http://paperhub.s3.amazonaws.com/f516fdfa940caa08c679d3946b273128.pdf) — было когда-то у Amazon
1. [Scalable Weakly-consistent Infection-style Process Group Membership Protocol](https://research.cs.cornell.edu/projects/Quicksilver/public_pdfs/SWIM.pdf)
1. [SWIM in Tarantool](https://slides.com/gerold103/swim-fosdem2020)
1. [Video: Making Gossip More Robust with Lifeguard](https://www.youtube.com/watch?v=u-a7rVJ6jZY&feature=youtu.be)
1. [Lifeguard: Failure Detection in the Era of Gray Failures](https://www.hashicorp.com/resources/failure-detection-in-the-era-of-gray-failures)
1. [Lifeguard: Local Health Awareness for More Accurate Failure Detection](https://arxiv.org/pdf/1707.00788.pdf)
